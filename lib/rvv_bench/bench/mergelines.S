#if 0
size_t
mergelines_rvv_vslide(char *str, size_t len)
{
	uint8_t *dest = (uint8_t*)str;
	uint8_t *src = (uint8_t*)str;
	char last = 0;

	vuint8m8_t v, u, d;
	vbool1_t m;

	for (size_t vl, VL; len > 1; ) {
		VL = vl = __riscv_vsetvl_e8m8(len);

		char next = len > vl ? src[vl] : 0;
		v = __riscv_vle8_v_u8m8(src, vl);
		u = __riscv_vslide1up_vx_u8m8(v, last, vl);
		d = __riscv_vslide1down_vx_u8m8(v, next, vl);

		m = __riscv_vmor_mm_b1(__riscv_vmsne_vx_u8m8_b1(u, '\\', vl), __riscv_vmsne_vx_u8m8_b1(v, '\n', vl), vl);
	#if DO_SKIP
		if (likely(__riscv_vcpop_m_b1(m, vl) == vl && next != '\n'))
			goto skip;
	#endif
		m = __riscv_vmand_mm_b1(
			m,
			__riscv_vmor_mm_b1(__riscv_vmsne_vx_u8m8_b1(v, '\\', vl), __riscv_vmsne_vx_u8m8_b1(d, '\n', vl), vl),
			vl);

		v = __riscv_vcompress_vm_u8m8(v, m, vl);
		vl = __riscv_vcpop_m_b1(m, vl);
	skip:
		__riscv_vse8_v_u8m8(dest, v, vl);
		dest += vl; src += VL; len -= VL;
		last = src[-1];
	}

	if (len > 0 && !(last == '\\' && *src == '\n')) *dest++ = *src++;
	return (dest - (uint8_t*)str);
}

size_t
mergelines_rvv_mshift(char *str, size_t count)
{
	if (count < 2) return count;
	uint8_t *dest = (uint8_t*)str;
	uint8_t *src = 1+(uint8_t*)str;
	char last = src[-1];
	size_t len = count-1;

	vuint8m8_t v, u, d;
	vbool1_t m;

	for (size_t vl, VL; len > 0; dest += vl, src += VL, len -= VL, last = src[-1]) {
		vl = VL = __riscv_vsetvl_e8m8(len);

		v = __riscv_vle8_v_u8m8(src, vl);
		u = __riscv_vslide1up_vx_u8m8(v, last, vl);

		m = __riscv_vmor_mm_b1(
				__riscv_vmsne_vx_u8m8_b1(u, '\\', vl),
				__riscv_vmsne_vx_u8m8_b1(v, '\n', vl), vl);
	#if DO_SKIP
		if (__riscv_vcpop_m_b1(m, vl) == vl) goto skip;
	#endif

		vuint8m1_t m1 = __riscv_vreinterpret_v_b1_u8m1(m);
		size_t vlmax8 = __riscv_vsetvlmax_e8m1();
		m1 = __riscv_vor_vv_u8m1(
			__riscv_vsrl_vx_u8m1(__riscv_vslide1up_vx_u8m1(m1, 0xFF, vlmax8), 7, vlmax8),
			__riscv_vsll_vx_u8m1(m1, 1, vlmax8), vlmax8);
		m = __riscv_vmand_mm_b1(m, __riscv_vreinterpret_v_u8m1_b1(m1), vl);

		u = __riscv_vcompress_vm_u8m8(u, m, vl);

		vl = __riscv_vcpop_m_b1(m, vl);
		VL += (VL^vl)&1&(VL < len); // missing bit in mask, so skip 1
skip:
		__riscv_vse8_v_u8m8(dest, u, vl);
	}
	if (count > 1 && !(src[-2] == '\\' && src[-1] == '\n')) *dest++ = last;
	return (dest - (uint8_t*)str);
}

#endif

#ifdef MX

.global MX(mergelines_rvv_vslide_) # generated by clang
MX(mergelines_rvv_vslide_):
	li a2, 2
	bltu a1, a2, MX(rvv_6)
	li t0, 0
	li a7, 92
	li a6, 1
	mv a2, a0
	mv a4, a0
	j MX(rvv_4)
MX(rvv_2):
	add a3, a4, a5
	lbu t1, 0(a3)
MX(rvv_3):
	vle8.v v8, (a4)
	add a3, a4, a5
	vslide1up.vx v16, v8, t0
	vslide1down.vx v24, v8, t1
	vmsne.vx v0, v16, a7
	vmsne.vi v16, v8, 10
	vmor.mm v16, v0, v16
	vmsne.vx v17, v8, a7
	vmsne.vi v18, v24, 10
	vmor.mm v17, v17, v18
	vmand.mm v16, v16, v17
	vcompress.vm v24, v8, v16
	vcpop.m a4, v16
	vsetvli zero, a4, e8, MX(), ta, ma
	vse8.v v24, (a2)
	lbu t0, -1(a3)
	sub a1, a1, a5
	add a2, a2, a4
	mv a4, a3
	bgeu a6, a1, MX(rvv_8)
MX(rvv_4):
	vsetvli a5, a1, e8, MX(), ta, ma
	bltu a5, a1, MX(rvv_2)
	li t1, 0
	j MX(rvv_3)
MX(rvv_6):
	mv a2, a0
	beqz a1, MX(rvv_10)
	lbu a1, 0(a0)
	mv a2, a0
	j MX(rvv_11)
MX(rvv_8):
	beqz a1, MX(rvv_10)
	lbu a1, 0(a3)
	xori a3, t0, 92
	xori a4, a1, 10
	or a3, a3, a4
	bnez a3, MX(rvv_11)
MX(rvv_10):
	sub a0, a2, a0
	ret
MX(rvv_11):
	addi a3, a2, 1
	sb a1, 0(a2)
	sub a0, a3, a0
	ret


.global MX(mergelines_rvv_vslide_skip_) # generated by clang
MX(mergelines_rvv_vslide_skip_):
	li a2, 2
	bltu a1, a2, MX(rvv_skip_9)
	li a5, 0
	li a6, 92
	li a7, 1
	mv t1, a0
	mv a3, a0
MX(rvv_skip_2):
	vsetvli a4, a1, e8, MX(), ta, ma
	bgeu a4, a1, MX(rvv_skip_4)
	add a2, a3, a4
	lbu t0, 0(a2)
	j MX(rvv_skip_5)
MX(rvv_skip_4):
	li t0, 0
MX(rvv_skip_5):
	vle8.v v8, (a3)
	vslide1up.vx v16, v8, a5
	vmsne.vx v24, v16, a6
	vmsne.vi v16, v8, 10
	vmor.mm v16, v24, v16
	vcpop.m a2, v16
	xor a2, a2, a4
	seqz a2, a2
	addi a5, t0, -10
	snez a5, a5
	and a2, a2, a5
	beqz a2, MX(rvv_skip_8)
	mv a2, a4
MX(rvv_skip_7):
	add a3, a3, a4
	vsetvli zero, a2, e8, MX(), ta, ma
	vse8.v v8, (t1)
	lbu a5, -1(a3)
	sub a1, a1, a4
	add t1, t1, a2
	bltu a7, a1, MX(rvv_skip_2)
	j MX(rvv_skip_11)
MX(rvv_skip_8):
	vslide1down.vx v24, v8, t0
	vmsne.vx v17, v8, a6
	vmsne.vi v18, v24, 10
	vmor.mm v17, v17, v18
	vmand.mm v16, v16, v17
	vcompress.vm v24, v8, v16
	vcpop.m a2, v16
	vmv.v.v v8, v24
	j MX(rvv_skip_7)
MX(rvv_skip_9):
	mv t1, a0
	beqz a1, MX(rvv_skip_13)
	lbu a1, 0(a0)
	mv t1, a0
	j MX(rvv_skip_14)
MX(rvv_skip_11):
	beqz a1, MX(rvv_skip_13)
	lbu a1, 0(a3)
	xori a2, a5, 92
	xori a3, a1, 10
	or a2, a2, a3
	bnez a2, MX(rvv_skip_14)
MX(rvv_skip_13):
	sub a0, t1, a0
	ret
MX(rvv_skip_14):
	addi a2, t1, 1
	sb a1, 0(t1)
	sub a0, a2, a0
	ret

.global MX(mergelines_rvv_mshift_)
MX(mergelines_rvv_mshift_):
	li a2, 2
	bltu a1, a2, 1f
	addi a2, a0, 1
	addi a3, a1, -1
	lbu a4, 0(a0)
	li a6, 92
	vsetvli a1, zero, e8, MXf8e8(), ta, ma
	li a7, -1
	mv t0, a0
2:
	vsetvli a5, a3, e8, MX(), ta, ma
	vle8.v v16, (a2)
	vslide1up.vx v8, v16, a4
	vmsne.vx v24, v8, a6
	vmsne.vi v25, v16, 10
	vmor.mm v16, v24, v25
	vsetvli a4, zero, e8, MXf8e8(), ta, ma
	vslide1up.vx v17, v16, a7
	vsrl.vi v17, v17, 7
	vadd.vv v18, v16, v16
	vor.vv v17, v17, v18
	vsetvli zero, a5, e8, MX(), ta, ma
	vmand.mm v16, v16, v17
	vcompress.vm v24, v8, v16
	vcpop.m a1, v16
	xor t1, a1, a5
	sltu a4, a5, a3
	and a4, a4, t1
	add a5, a5, a4
	vsetvli zero, a1, e8, MX(), ta, ma
	vse8.v v24, (t0)
	add a2, a2, a5
	lbu a4, -1(a2)
	sub a3, a3, a5
	add t0, t0, a1
	bnez a3, 2b
	lbu a1, -2(a2)
	li a3, 92
	bne a1, a3, 3f
	lbu a1, -1(a2)
	li a2, 10
	beq a1, a2, 4f
3:
	addi a1, t0, 1
	sb a4, 0(t0)
	mv t0, a1
4:
	sub a1, t0, a0
1:
	mv a0, a1
	ret

.global MX(mergelines_rvv_mshift_skip_)
MX(mergelines_rvv_mshift_skip_):
	li a2, 2
	bltu a1, a2, 1f
	addi a2, a0, 1
	addi a3, a1, -1
	lbu t0, 0(a0)
	li a7, 92
	vsetvli a1, zero, e8, MXf8e8(), ta, ma
	li a6, -1
	mv a5, a0
	j 4f
2:
	vsetvli a4, zero, e8, MXf8e8(), ta, ma
	vslide1up.vx v17, v16, a6
	vsrl.vi v17, v17, 7
	vadd.vv v18, v16, v16
	vor.vv v17, v17, v18
	vsetvli zero, a1, e8, MX(), ta, ma
	vmand.mm v16, v16, v17
	vcompress.vm v24, v8, v16
	vcpop.m t0, v16
	xor t1, t0, a1
	sltu a4, a1, a3
	and a4, a4, t1
	add a4, a4, a1
	mv a1, t0
	vmv.v.v v8, v24
3:
	vsetvli zero, a1, e8, MX(), ta, ma
	vse8.v v8, (a5)
	add a2, a2, a4
	lbu t0, -1(a2)
	sub a3, a3, a4
	add a5, a5, a1
	beqz a3, 5f
4:
	vsetvli a1, a3, e8, MX(), ta, ma
	vle8.v v16, (a2)
	vslide1up.vx v8, v16, t0
	vmsne.vx v24, v8, a7
	vmsne.vi v25, v16, 10
	vmor.mm v16, v24, v25
	vcpop.m a4, v16
	bne a4, a1, 2b
	mv a4, a1
	j 3b
5:
	lbu a1, -2(a2)
	li a3, 92
	bne a1, a3, 6f
	lbu a1, -1(a2)
	li a2, 10
	beq a1, a2, 7f
6:
	addi a1, a5, 1
	sb t0, 0(a5)
	mv a5, a1
7:
	sub a1, a5, a0
1:
	mv a0, a1
	ret


#endif
