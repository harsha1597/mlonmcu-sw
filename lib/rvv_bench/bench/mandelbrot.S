#if 0

void
mandelbrot_rvv(size_t width, size_t maxIter, uint32_t *res)
{
	size_t VL2 = __riscv_vsetvlmax_e32m2();
	vfloat32m2_t v4 = __riscv_vfmv_v_f_f32m2(4, VL2);
	vuint32m2_t vid = __riscv_vid_v_u32m2(VL2);
	vfloat32m2_t cx, cy, zx, zy, zx2, zy2;

	for (size_t y = 0; y < width; ++y) {
		cy = __riscv_vfmv_v_f_f32m2(y, VL2);
		cy = __riscv_vfadd(__riscv_vfmul(cy, 2.0f / width, VL2), -1, VL2);

		for (size_t vl, x = 0, n = width; n > 0; n -= vl, res += vl, x += vl) {
			vl = __riscv_vsetvl_e32m2(n);

			cx = __riscv_vfcvt_f(__riscv_vadd(vid, x, vl), vl);
			cx = __riscv_vfadd(__riscv_vfmul(cx, 2.0f / width, vl), -1.5f, vl);

			size_t iter = 0;
			vuint32m2_t viter = __riscv_vmv_v_x_u32m2(0, vl);
			vbool16_t mask = __riscv_vmset_m_b16(vl);
			zx = zy = zx2 = zy2 = __riscv_vfmv_v_f_f32m2(0, vl);
			do {
				mask = __riscv_vmflt(__riscv_vfadd(zx2, zy2, vl), v4, vl);
				viter = __riscv_vadc(viter, 0, mask, vl);
				zy = __riscv_vfmacc(cy, __riscv_vfadd(zx, zx, vl), zy, vl);
				zx = __riscv_vfadd(__riscv_vfsub(zx2, zy2, vl), cx, vl);
				zx2 = __riscv_vfmul(zx, zx, vl);
				zy2 = __riscv_vfmul(zy, zy, vl);
				++iter;
			} while (iter < maxIter && __riscv_vfirst(mask, vl) >= 0);
			__riscv_vse32(res, viter, vl);
		}
	}
}
#endif

#if MX_N > 0 && MX_N <= 2

#if IF_VF16(1)+0
.global MX(mandelbrot_rvv_f16_) # generated by clang
.balign 2
MX(rvv_f16_m1p5):
	.half 0xbe00 # half -1.5
MX(rvv_f16_m1):
	.half 0xbc00 # half -1
MX(rvv_f16_p4):
	.half 0x4400 # half 4
MX(mandelbrot_rvv_f16_):
	beqz a0, 9f
	li a6, 0
	vsetvli a3, zero, e16, m2, ta, ma
	la a3, MX(rvv_f16_p4)
	fcvt.s.wu fa5, a0
	flh fa4, (a3)
	lui a3, 262144
	fmv.w.x fa3, a3
	la a3, MX(rvv_f16_m1)
	fdiv.s fa3, fa3, fa5
	flh fa5, (a3)
	la a3, MX(rvv_f16_m1p5)
	vfmv.v.f v12, fa4
	flh fa4, (a3)
	addi a3, a1, -1
	sltu a1, a1, a3
	addi a1, a1, -1
	and a1, a1, a3
	vid.v v14
	fcvt.h.s fa3, fa3
	addi a7, a1, 1
	j 2f
1:
	addi a6, a6, 1
	beq a6, a0, 9f
2:
	li a4, 0
	fcvt.h.wu fa2, a6
	vsetvli a1, zero, e16, m2, ta, ma
	vfmv.v.f v8, fa2
	vfmul.vf v8, v8, fa3
	vfadd.vf v16, v8, fa5
	mv a5, a0
	j 4f
3:
	vsetvli zero, zero, e32, m4, ta, ma
	vse32.v v8, (a2)
	sub a5, a5, t0
	slli a1, t0, 2
	add a2, a2, a1
	add a4, a4, t0
	beqz a5, 1b
4:
	vsetvli t0, a5, e16, m2, ta, ma
	vadd.vx v8, v14, a4
	vfcvt.f.xu.v v8, v8
	vfmul.vf v8, v8, fa3
	vfadd.vf v18, v8, fa4
	vmv.v.i v20, 0
	vmv.v.i v22, 0
	vmv.v.i v24, 0
	vmv.v.i v26, 0
	vsetvli zero, zero, e32, m4, ta, ma
	vmv.v.i v8, 0
	mv a1, a7
5:
	vsetvli zero, zero, e16, m2, ta, ma
	vfadd.vv v28, v24, v20
	vmflt.vv v0, v28, v12
	addi a1, a1, -1
	vsetvli zero, zero, e32, m4, ta, ma
	vadc.vim v8, v8, 0, v0
	beqz a1, 3b
	vsetvli zero, zero, e16, m2, ta, ma
	vfadd.vv v26, v26, v26
	vfsub.vv v20, v24, v20
	vfmadd.vv v22, v26, v16
	vfadd.vv v26, v20, v18
	vfmul.vv v20, v22, v22
	vfirst.m a3, v0
	vfmul.vv v24, v26, v26
	bgez a3, 5b
	j 3b
9:
	ret
#endif

.global MX(mandelbrot_rvv_f32_) # generated by clang
MX(mandelbrot_rvv_f32_):
	beqz a0, 9f
	li a6, 0
	vsetvli a3, zero, e32, MX(), ta, ma
	lui a3, 264192
	fcvt.s.wu fa5, a0
	vmv.v.x v8, a3
	lui a3, 262144
	fmv.w.x fa4, a3
	fdiv.s fa5, fa4, fa5
	addi a3, a1, -1
	sltu a1, a1, a3
	addi a1, a1, -1
	and a1, a1, a3
	lui a3, 784384
	fmv.w.x fa4, a3
	lui a3, 785408
	fmv.w.x fa3, a3
	vid.v v10
	addi a7, a1, 1
	j 2f
1:
	addi a6, a6, 1
	beq a6, a0, 9f
2:
	li a4, 0
	fcvt.s.wu fa2, a6
	vsetvli a1, zero, e32, MX(), ta, ma
	vfmv.v.f v12, fa2
	vfmul.vf v12, v12, fa5
	vfadd.vf v12, v12, fa4
	mv a5, a0
	j 4f
3:
	vse32.v v14, (a2)
	sub a5, a5, t0
	slli a1, t0, 2
	add a2, a2, a1
	add a4, a4, t0
	beqz a5, 1b
4:
	vsetvli t0, a5, e32, MX(), ta, ma
	vadd.vx v14, v10, a4
	vmv.v.i v18, 0
	vfcvt.f.xu.v v14, v14
	vfmul.vf v14, v14, fa5
	vfadd.vf v16, v14, fa3
	vmv.v.i v14, 0
	mv a1, a7
	vmv.v.i v22, 0
	vmv.v.i v20, 0
	vmv.v.i v24, 0
5:
	vfadd.vv v26, v22, v18
	vmflt.vv v0, v26, v8
	addi a1, a1, -1
	vadc.vim v14, v14, 0, v0
	beqz a1, 3b
	vfadd.vv v24, v24, v24
	vfsub.vv v18, v22, v18
	vfmadd.vv v20, v24, v12
	vfadd.vv v24, v18, v16
	vfmul.vv v18, v20, v20
	vfirst.m a3, v0
	vfmul.vv v22, v24, v24
	bgez a3, 5b
	j 3b
9:
	ret

#if IF_VF64(1)+0
.balign 8
.global MX(mandelbrot_rvv_f64_) # generated by clang
MX(rvv_f64_m1p5):
	.quad 0xbff8000000000000 # double -1.5
MX(rvv_f64_m1):
	.quad 0xbff0000000000000 # double -1
MX(rvv_f64_p4):
	.quad 0x4010000000000000 # double 4
MX(mandelbrot_rvv_f64_):
	beqz a0, 9f
	li a6, 0
	vsetvli a3, zero, e64, m2, ta, ma
	la a3, MX(rvv_f64_p4)
	fcvt.s.wu fa5, a0
	fld fa4, (a3)
	lui a3, 262144
	fmv.w.x fa3, a3
	la a3, MX(rvv_f64_m1)
	fdiv.s fa3, fa3, fa5
	fld fa5, (a3)
	la a3, MX(rvv_f64_m1p5)
	vfmv.v.f v8, fa4
	fld fa4, (a3)
	addi a3, a1, -1
	sltu a1, a1, a3
	addi a1, a1, -1
	and a1, a1, a3
	vid.v v10
	fcvt.d.s fa3, fa3
	addi a7, a1, 1
	j 2f
1:
	addi a6, a6, 1
	beq a6, a0, 9f
2:
	li a4, 0
	fcvt.d.wu fa2, a6
	vsetvli a1, zero, e64, m2, ta, ma
	vfmv.v.f v12, fa2
	vfmul.vf v12, v12, fa3
	vfadd.vf v12, v12, fa5
	mv a5, a0
	j 4f
3:
	vsetvli zero, zero, e32, m1, ta, ma
	vse32.v v24, (a2)
	sub a5, a5, t0
	slli a1, t0, 2
	add a2, a2, a1
	add a4, a4, t0
	beqz a5, 1b
4:
	vsetvli t0, a5, e64, m2, ta, ma
	vadd.vx v14, v10, a4
	vfcvt.f.xu.v v14, v14
	vfmul.vf v14, v14, fa3
	vfadd.vf v14, v14, fa4
	vmv.v.i v16, 0
	vmv.v.i v18, 0
	vmv.v.i v20, 0
	vmv.v.i v22, 0
	vsetvli zero, zero, e32, m1, ta, ma
	vmv.v.i v24, 0
	mv a1, a7
5:
	vsetvli zero, zero, e64, m2, ta, ma
	vfadd.vv v26, v20, v16
	vmflt.vv v0, v26, v8
	addi a1, a1, -1
	vsetvli zero, zero, e32, m1, ta, ma
	vadc.vim v24, v24, 0, v0
	beqz a1, 3b
	vsetvli zero, zero, e64, m2, ta, ma
	vfadd.vv v22, v22, v22
	vfsub.vv v16, v20, v16
	vfmadd.vv v18, v22, v12
	vfadd.vv v22, v16, v14
	vfmul.vv v16, v18, v18
	vfirst.m a3, v0
	vfmul.vv v20, v22, v22
	bgez a3, 5b
	j 3b
9:
	ret
#endif

#endif


